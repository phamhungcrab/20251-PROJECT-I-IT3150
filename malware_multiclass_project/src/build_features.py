from __future__ import annotations

from pathlib import Path
from typing import List, Tuple, Dict, Optional

import numpy as np
import pandas as pd
import scipy.sparse as sp

from .config import Config
from .utils import human_bytes, df_memory_bytes, sparse_memory_bytes


REQ_COLS = ("sha256", "type")


def _require_cols(df: pd.DataFrame, name: str) -> None:
    cols = set(df.columns)
    missing = [c for c in REQ_COLS if c not in cols]
    if missing:
        raise ValueError(f"[{name}] Missing required columns: {missing}. Available: {list(df.columns)[:10]}...")


def _drop_duplicates_by_sha(df: pd.DataFrame, name: str, logger=None) -> pd.DataFrame:
    before = len(df)
    df2 = df.drop_duplicates(subset=["sha256"], keep="first").copy()
    after = len(df2)
    if logger and after != before:
        logger.warning(f"[{name}] Duplicates by sha256: dropped {before-after} rows (kept first).")
    return df2


def _prefix_features(df: pd.DataFrame, prefix: str) -> pd.DataFrame:
    """
    Prefix tất cả feature columns để tránh trùng tên khi stack.
    """
    feat_cols = [c for c in df.columns if c not in ("sha256", "type")]
    rename = {c: f"{prefix}__{c}" for c in feat_cols}
    return df.rename(columns=rename)


def align_intersection_sha(
    dlls: pd.DataFrame,
    api: pd.DataFrame,
    hdr: pd.DataFrame,
    sect: pd.DataFrame,
    logger=None
) -> List[str]:
    """
    Lấy intersection của sha256 giữa 4 file để đảm bảo mỗi sample có đủ 4 nhóm feature.
    """
    s1 = set(dlls["sha256"].astype(str))
    s2 = set(api["sha256"].astype(str))
    s3 = set(hdr["sha256"].astype(str))
    s4 = set(sect["sha256"].astype(str))
    inter = s1 & s2 & s3 & s4

    if logger:
        logger.info(f"sha256 counts: dlls={len(s1):,} api={len(s2):,} hdr={len(s3):,} sect={len(s4):,}")
        logger.info(f"intersection sha256 = {len(inter):,} samples (will be used).")

        # report missing
        logger.info(f"Missing relative to intersection:")
        logger.info(f"  - dlls missing: {len(inter - s1):,}")
        logger.info(f"  - api  missing: {len(inter - s2):,}")
        logger.info(f"  - hdr  missing: {len(inter - s3):,}")
        logger.info(f"  - sect missing: {len(inter - s4):,}")

    sha_order = sorted(inter)
    return sha_order


def build_sparse_Xy(
    dlls: pd.DataFrame,
    api: pd.DataFrame,
    hdr: pd.DataFrame,
    sect: pd.DataFrame,
    logger=None
) -> Tuple[sp.csr_matrix, np.ndarray, np.ndarray, List[str]]:
    """
    Align theo sha256 intersection và build:
      - X: scipy CSR sparse matrix
      - y: nhãn (string) theo thứ tự sha
      - sha: mảng sha256 theo thứ tự
      - feature_names: list tên feature theo thứ tự cột X
    """
    _require_cols(dlls, "dlls")
    _require_cols(api, "api")
    _require_cols(hdr, "hdr")
    _require_cols(sect, "sect")

    # đảm bảo sha256 là string
    for df in (dlls, api, hdr, sect):
        df["sha256"] = df["sha256"].astype(str)

    # drop duplicates
    dlls = _drop_duplicates_by_sha(dlls, "dlls", logger)
    api = _drop_duplicates_by_sha(api, "api", logger)
    hdr = _drop_duplicates_by_sha(hdr, "hdr", logger)
    sect = _drop_duplicates_by_sha(sect, "sect", logger)

    sha_order = align_intersection_sha(dlls, api, hdr, sect, logger)
    sha_arr = np.array(sha_order, dtype=object)

    # prefix feature names để tránh trùng
    dlls = _prefix_features(dlls, "dlls")
    api = _prefix_features(api, "api")
    hdr = _prefix_features(hdr, "hdr")
    sect = _prefix_features(sect, "sect")

    # align rows theo sha_order
    def _align(df: pd.DataFrame, name: str) -> pd.DataFrame:
        df = df.set_index("sha256", drop=False)
        # loc theo sha_order (đảm bảo thứ tự)
        aligned = df.loc[sha_order]
        aligned = aligned.reset_index(drop=True)
        if logger:
            logger.info(f"Aligned {name}: shape={aligned.shape}, mem={human_bytes(df_memory_bytes(aligned))}")
        return aligned

    dlls_a = _align(dlls, "dlls")
    api_a = _align(api, "api")
    hdr_a = _align(hdr, "hdr")
    sect_a = _align(sect, "sect")

    # --- Label consistency check ---
    y_cols = [
        dlls_a["type"].astype(str).to_numpy(),
        api_a["type"].astype(str).to_numpy(),
        hdr_a["type"].astype(str).to_numpy(),
        sect_a["type"].astype(str).to_numpy(),
    ]
    y_stack = np.vstack(y_cols).T  # (n,4)
    # majority vote (mode)
    y_mode = []
    mismatch_rows = 0
    for row in y_stack:
        vals, cnts = np.unique(row, return_counts=True)
        best = vals[np.argmax(cnts)]
        y_mode.append(best)
        if len(vals) > 1:
            mismatch_rows += 1
    y = np.array(y_mode, dtype=object)

    if logger:
        logger.info(f"Label mismatch rows across files: {mismatch_rows:,} / {len(y):,}")
        if mismatch_rows > 0:
            # show a few examples
            idxs = np.where(np.apply_along_axis(lambda r: len(set(r))>1, 1, y_stack))[0][:5]
            for i in idxs:
                logger.warning(f"  mismatch example sha={sha_arr[i]} labels={y_stack[i].tolist()} -> using '{y[i]}'")

    # --- Build sparse X ---
    def _to_csr_features(df: pd.DataFrame, name: str) -> Tuple[sp.csr_matrix, List[str]]:
        feat_cols = [c for c in df.columns if c not in ("sha256", "type")]
        X_dense = df[feat_cols].to_numpy(copy=False)  # giữ dtype hiện có
        # CSR from dense; giữ dtype, sau đó cast float32 trên nnz để tiết kiệm RAM
        X = sp.csr_matrix(X_dense)
        X = X.astype(np.float32)
        if logger:
            nnz = X.nnz
            logger.info(
                f"{name}: features={len(feat_cols):,} X.shape={X.shape} nnz={nnz:,} mem={human_bytes(sparse_memory_bytes(X))}"
            )
        return X, feat_cols

    # convert numeric cols in hdr/sect to numeric & fill NaN with 0 (không leak vì fill constant)
    for df_name, df in (("hdr", hdr_a), ("sect", sect_a)):
        feat_cols = [c for c in df.columns if c not in ("sha256", "type")]
        for c in feat_cols:
            if not pd.api.types.is_numeric_dtype(df[c]):
                df[c] = pd.to_numeric(df[c], errors="coerce")
        df[feat_cols] = df[feat_cols].fillna(0)

    X_dlls, fn_dlls = _to_csr_features(dlls_a, "DLLs")
    X_api, fn_api = _to_csr_features(api_a, "APIs")
    X_hdr, fn_hdr = _to_csr_features(hdr_a, "Header")
    X_sect, fn_sect = _to_csr_features(sect_a, "Section")

    X = sp.hstack([X_dlls, X_api, X_hdr, X_sect], format="csr").astype(np.float32)
    feature_names = fn_dlls + fn_api + fn_hdr + fn_sect

    if logger:
        logger.info(f"FINAL X: shape={X.shape}, nnz={X.nnz:,}, mem={human_bytes(sparse_memory_bytes(X))}")
        logger.info(f"FINAL y: shape={y.shape}, unique_classes={len(pd.unique(y))}")

    return X, y, sha_arr, feature_names


def processed_paths(cfg: Config) -> Dict[str, Path]:
    pdir = Path(cfg.data_dir) / cfg.processed_dirname
    return {
        "dir": pdir,
        "X": pdir / cfg.processed_X_name,
        "y": pdir / cfg.processed_y_name,
        "sha": pdir / cfg.processed_sha_name,
        "feat": pdir / cfg.processed_feat_name,
        "classes": pdir / cfg.processed_classes_name,
    }


def save_processed(cfg: Config, X: sp.csr_matrix, y: np.ndarray, sha: np.ndarray, feature_names: List[str], logger=None) -> None:
    paths = processed_paths(cfg)
    paths["dir"].mkdir(parents=True, exist_ok=True)

    sp.save_npz(paths["X"], X)
    np.save(paths["y"], y, allow_pickle=True)
    np.save(paths["sha"], sha, allow_pickle=True)

    # feature_names json
    import json
    with paths["feat"].open("w", encoding="utf-8") as f:
        json.dump(feature_names, f, ensure_ascii=False, indent=2)

    if logger:
        logger.info(f"Saved processed dataset to: {paths['dir']}")


def load_processed(cfg: Config, logger=None) -> Optional[Tuple[sp.csr_matrix, np.ndarray, np.ndarray, List[str]]]:
    paths = processed_paths(cfg)
    if not paths["X"].exists() or not paths["y"].exists() or not paths["sha"].exists() or not paths["feat"].exists():
        return None

    X = sp.load_npz(paths["X"]).tocsr()
    y = np.load(paths["y"], allow_pickle=True)
    sha = np.load(paths["sha"], allow_pickle=True)

    import json
    with paths["feat"].open("r", encoding="utf-8") as f:
        feature_names = json.load(f)

    if logger:
        logger.info(f"Loaded processed dataset from: {paths['dir']}")
        logger.info(f"X.shape={X.shape} nnz={X.nnz:,} mem={human_bytes(sparse_memory_bytes(X))}")
        logger.info(f"y.shape={y.shape} unique={len(pd.unique(y))}")

    return X, y, sha, feature_names
